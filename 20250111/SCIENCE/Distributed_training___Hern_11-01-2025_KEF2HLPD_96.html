<!doctype html>
<html class="no-js">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>I can do it with a distributed heart</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
    <link rel="stylesheet" href="../theme/styles/main.css">
    <meta name="format-detection" content="telephone=no">
  <link rel="manifest" href="/manifest.json">
<meta name='robots' content='noindex,follow' />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://main-ecnpaper-economist.content.pugpig.com/news/20250111/SCIENCE/Distributed_training___Hern_11-01-2025_KEF2HLPD_96.html" />
<meta property="article:published_time" content="2025-01-11 00:00:00" />
<meta property="article:modified_time" content="2025-01-09 18:59:38" />
<meta property="article:section" content="Science and technology" />
<meta property="og:title" content="I can do it with a distributed heart" />
<meta property="og:description" content="Training AI models might not need enormous data centres" />
<meta property="og:audio" content="https://main-ecnpaper-economist.content.pugpig.com/news/20250111/audio/SCIENCE_KEF2HLPD.mp3" />
<meta property="og:audio:type" content="audio/mpeg" />
<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@graph": [
        {
            "id": "Distributed_training___Hern_11-01-2025_KEF2HLPD_96.html",
            "type": "Article",
            "articleId": "20250111_article_94809",
            "articleSection": "Science and technology",
            "author": "",
            "schema:dateModified": "2025-01-09T18:59:38+00:00",
            "schema:datePublished": "2025-01-11T00:00:00+00:00",
            "description": "Training AI models might not need enormous data centres",
            "headline": "I can do it with a distributed heart",
            "schema:url": "https://main-ecnpaper-economist.content.pugpig.com/news/20250111/SCIENCE/Distributed_training___Hern_11-01-2025_KEF2HLPD_96.html"
        },
        {
            "id": "_:b0",
            "type": "Periodical",
            "name": "The Economist Newspaper",
            "publisher": "Economist Group"
        },
        {
            "id": "_:b1",
            "type": "PublicationIssue",
            "schema:dateModified": "2025-01-09T18:59:38+00:00",
            "schema:datePublished": "2025-01-11T00:00:00+00:00",
            "description": "The world\u2019s poorest continent should embrace its least fashionable idea",
            "editionId": "20250111",
            "issueNumber": "2025-01-11",
            "name": "11th Jan 2025"
        }
    ]
}
</script><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/" />
<meta name="DCTERMS.isPartOf" content="20250111" />
<link rel="DCTERMS.isRequiredBy" href="/endpoint.xml" />

<link rel="manifest" href="/manifest.json">
<meta name='robots' content='noindex,follow' />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://main-ecnpaper-economist.content.pugpig.com/news/20250111/SCIENCE/Distributed_training___Hern_11-01-2025_KEF2HLPD_96.html" />
<meta property="article:published_time" content="2025-01-11 00:00:00" />
<meta property="article:modified_time" content="2025-01-09 18:59:38" />
<meta property="article:section" content="Science and technology" />
<meta property="og:image" content="https://main-ecnpaper-economist.content.pugpig.com/pp-thumb-26cf9bf46cc90ebcad74baff61ecd1fe.jpg" />
<meta property="og:image:type" content="image/jpeg" />
<meta property="og:image:width" content="256" />
<meta property="og:image:height" content="341" />
<meta property="og:title" content="I can do it with a distributed heart" />
<meta property="og:description" content="Training AI models might not need enormous data centres" />
<meta property="og:audio" content="https://main-ecnpaper-economist.content.pugpig.com/news/20250111/audio/SCIENCE_KEF2HLPD.mp3" />
<meta property="og:audio:type" content="audio/mpeg" />
<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@graph": [
        {
            "id": "Distributed_training___Hern_11-01-2025_KEF2HLPD_96.html",
            "type": "Article",
            "articleId": "20250111_article_94809",
            "articleSection": "Science and technology",
            "author": "",
            "schema:dateModified": "2025-01-09T18:59:38+00:00",
            "schema:datePublished": "2025-01-11T00:00:00+00:00",
            "description": "Training AI models might not need enormous data centres",
            "headline": "I can do it with a distributed heart",
            "schema:image": "https://main-ecnpaper-economist.content.pugpig.com/pp-thumb-26cf9bf46cc90ebcad74baff61ecd1fe.jpg",
            "thumbnail:Url": "https://main-ecnpaper-economist.content.pugpig.com/pp-thumb-26cf9bf46cc90ebcad74baff61ecd1fe.jpg",
            "schema:url": "https://main-ecnpaper-economist.content.pugpig.com/news/20250111/SCIENCE/Distributed_training___Hern_11-01-2025_KEF2HLPD_96.html"
        },
        {
            "id": "_:b0",
            "type": "Periodical",
            "name": "The Economist Newspaper",
            "publisher": "Economist Group"
        },
        {
            "id": "_:b1",
            "type": "PublicationIssue",
            "schema:dateModified": "2025-01-09T18:59:38+00:00",
            "schema:datePublished": "2025-01-11T00:00:00+00:00",
            "description": "The world\u2019s poorest continent should embrace its least fashionable idea",
            "editionId": "20250111",
            "issueNumber": "2025-01-11",
            "name": "11th Jan 2025"
        }
    ]
}
</script><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/" />
<meta name="DCTERMS.isPartOf" content="20250111" />
<link rel="DCTERMS.isRequiredBy" href="/endpoint.xml" />
<script>
    if (/^http/.test(window.location.protocol)
            && /[?&]preview(?:=(?:true|1))?(?:$|&)/ig.test(location.search) === false) {
        var script = document.createElement('script');
        script.setAttribute('class', 'KGPugpigReader-bootstrap');
        script.setAttribute('src', '/pugpig-websmartbanner.js');
        document.head.appendChild(script);
    }
</script>
</head>
  <body class="article-01 content-wide section-science">
    <div class="section-header">
      <h3 class="section-header__title">Science and technology</h3>
    </div>

    <div class="container">
      <article class="article">
        <section class="article__header">
          <div class="header-group">
              <p class="fly">Artificial intelligence</p>

              <h1 class="headline">I can do it with a distributed heart</h1>


              <h2 class="standfirst">Training AI models might not need enormous data centres</h2>
          </div>

        </section>

        <section class="article__body">
          <figure class="media media--drawing media--wide"><div><img class="media__image" src="images/20250111_STD001.png"></div></figure><p><span data-caps="initial">O</span><small>NCE, THE</small> world&#8217;s richest men competed over yachts, jets and private islands. Now, the size-measuring contest of choice is clusters. Just 18 months ago, Open<small>AI</small> trained <small>GPT</small>-4, its then state-of-the-art large language model (<small>LLM</small>), on a network of around 25,000 then state-of-the-art graphics processing units (<small>GPU</small>s) made by Nvidia. Now Elon Musk and Mark Zuckerberg, bosses of X and Meta respectively, are waving their chips in the air: Mr Musk says he has 100,000 <small>GPU</small>s in one data centre and plans to buy 200,000. Mr Zuckerberg says he&#8217;ll get 350,000.</p><p>This contest to build ever-bigger computing clusters for ever-more-powerful artificial-intelligence (<small>AI</small>) models cannot continue indefinitely. Each extra chip adds not only processing power but also to the organisational burden of keeping the whole cluster synchronised. The more chips there are, the more time the data centre&#8217;s chips will spend shuttling data around rather than doing useful work. Simply increasing the number of <small>GPU</small>s will provide diminishing returns.</p><p>Computer scientists are therefore looking for cleverer, less resource-intensive ways to train future <small>AI</small> models. The solution could lie with ditching the enormous bespoke computing clusters (and their associated upfront costs) altogether and, instead, distributing the task of training between many smaller data centres. This, say some experts, could be the first step towards an even more ambitious goal&#8212;training <small>AI</small> models without the need for any dedicated hardware at all.</p><p>Training a modern <small>AI</small> system involves ingesting data&#8212;sentences, say, or the structure of a protein&#8212;that has had some sections hidden. The model makes a guess at what the hidden sections might contain. If it makes the wrong guess, the model is tweaked by a mathematical process called backpropagation so that, the next time it tries the same prediction, it will be infinitesimally closer to the correct answer.</p><h3>I knew you were trouble</h3><p>The problems come when you want to be able to work &#8220;in parallel&#8221;&#8212;to have two, or 200,000, <small>GPU</small>s working on backpropagation at the same time. After each step, the chips share data about the changes they have made. If they didn&#8217;t, you wouldn&#8217;t have a single training run, you&#8217;d have 200,000 chips training 200,000 models on their own. That step, called &#8220;checkpointing&#8221;, can get complicated fast. There is only one link between two chips, but 190 between 20 chips and almost 20bn for 200,000 chips. The time it takes to checkpoint grows commensurately. For big training runs, around half the time can often be spent on checkpointing.</p><p>All that wasted time gave Arthur Douillard, an engineer at Google DeepMind, an idea. Why not just do fewer checkpoints? In late 2023, he and his colleagues published a method for &#8220;Distributed Low-Communication Training of Language Models&#8221;, or DiLoCo. Rather than training on 100,000 <small>GPU</small>s, all of which speak to each other at every step, DiLoCo describes how to distribute training across different &#8220;islands&#8221;, each still a sizeable data centre. Within the islands, checkpointing continues as normal, but across them, the communication burden drops 500-fold.</p><p>There are trade-offs. Models trained this way seem to struggle to hit the same peak performance as those trained in monolithic data centres. But interestingly, that impact seems to exist only when the models are rated on the same tasks they are trained on: predicting the missing data.</p><p>When they are turned to predictions that they&#8217;ve never been asked to make before, they seem to generalise better. Ask them to answer a reasoning question in a form not in the training data, and pound for pound they may outclass the traditionally trained models. That could be an artefact of each island of compute being slightly freer to spiral off in its own direction between checkpointing runs, when they get hauled back on task. Like a cohort of studious undergraduates forming their own research groups rather than being lectured to en masse, the end result is therefore slightly less focused on the task at hand, but with a much wider experience.</p><p>Vincent Weisser, founder of Prime Intellect, an open-source <small>AI</small> lab, has taken DiLoCo and run with it. In November 2024 his team completed training on Intellect-1, a 10bn-parameter <small>LLM</small> comparable to Meta&#8217;s centrally trained Llama 2, which was state-of-the-art when released in 2023.</p><p>Mr Weisser&#8217;s team built OpenDiLoCo, a lightly modified version of Mr Douillard&#8217;s original, and set it to work training a new model using 30 <small>GPU</small> clusters in eight cities across three continents. In his trials, the <small>GPU</small>s ended up actively working for 83% of the time&#8212;that&#8217;s compared with 100% in the baseline scenario, in which all the <small>GPU</small>s were in the same building. When training was limited to data centres in America, they were actively working for 96% of the time. Instead of checkpointing every training step, Mr Weisser&#8217;s approach checkpoints only every 500 steps. And instead of sharing all the information about every change, it &#8220;quantises&#8221; the changes, dropping the least significant three-quarters of the data.</p><p>For the most advanced labs, with monolithic data centres already built, there is no pressing reason to make the switch to distributed training yet. But, given time, Mr Douillard thinks that his approach will become the norm. The advantages are clear, and the downsides&#8212;at least, those illustrated by the small training runs that have been completed so far&#8212;seem to be fairly limited.</p><p>For an open-source lab like Prime Intellect, the distributed approach has other benefits. Data centres big enough to train a 10bn-parameter model are few and far between. That scarcity drives up prices to access their compute&#8212;if it is even available on the open market at all, rather than hoarded by the companies that have built them. Smaller clusters are readily available, however. Each of the 30 clusters Prime Intellect used was a rack of just eight <small>GPU</small>s, with up to 14 of the clusters online at any given time. This resource is a thousand times smaller than data centres used by frontier labs, but neither Mr Weisser nor Mr Douillard see any reason why their approach would not scale.</p><p>For Mr Weisser, the motivation for distributing training is also to distribute power&#8212;and not just in the electrical sense. &#8220;It&#8217;s extremely important that it&#8217;s not in the hands of one nation, one corporation,&#8221; he says. The approach is hardly a free-for-all, though&#8212;one of the eight-<small>GPU</small> clusters he used in his training run costs $600,000; the total network deployed by Prime Intellect would cost $18m to buy. But his work is a sign, at least, that training capable <small>AI</small> models does not have to cost billions of dollars.</p><p>And what if the costs could drop further still? The dream for developers pursuing truly decentralised <small>AI</small> is to drop the need for purpose-built training chips entirely. Measured in teraflops, a count of how many operations a chip can do in a second, one of Nvidia&#8217;s most capable chips is roughly as powerful as 300 or so top-end iPhones. But there are a lot more iPhones in the world than <small>GPU</small>s. What if they (and other consumer computers) could all be put to work, churning through training runs while their owners sleep?</p><p>The trade-offs would be enormous. The ease of working with high-performance chips is that, even when distributed around the world, they are at least the same model operating at the same speed. That would be lost. Worse, not only would the training progress need to be aggregated and redistributed at each checkpoint step, so would the training data itself, since typical consumer hardware is unable to store the terabytes of data that goes into a cutting-edge <small>LLM</small>. New computing breakthroughs would be required, says Nic Lane of Flower, one of the labs trying to make that approach a reality.</p><p class="end has-stop">The gains, though, could add up, with the approach leading to better models, reckons Mr Lane. In the same way that distributed training makes models better at generalising, models trained on &#8220;sharded&#8221; datasets, where only portions of the training data are given to each <small>GPU</small>, could perform better when confronted with unexpected input in the real world. All that would leave the billionaires needing something else to compete over. </p>
        </section>
      </article>
    </div>
  </body>
</html>